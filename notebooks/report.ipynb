{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Abstract Classification\n",
    "\n",
    "In this project I decided to fetch my own data from arxiv with `arxiv api` and store them in the PSQL database on my local to save myself from `csv` files and create paper abstract classification webapp.\n",
    "\n",
    "App has to look like this:\n",
    "![](assets/website.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you choose an abstract to get prediction, it will take you to a new page with prediction.\n",
    "\n",
    "To open the web app first go to `backend/` and run \n",
    "\n",
    "`python main.py`.\n",
    "\n",
    "Then in another shell or terminal tab go to `frontend/app/` and run \n",
    "\n",
    "`uvicorn --host 0.0.0.0 --port 8080 main:app --reload`.\n",
    "\n",
    "Go to the link and voala!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `arxiv` library, I pulled 8000 data, 2000 from each of the `ai, cv, ml and ds` categories. The reason why I kept the number of data low was to not have many problems in terms of compute while working. I still had to train some models via kaggle. It was inevitable that I would have difficulties in model training, especially since each of our inputs consisted of long texts.\n",
    "\n",
    "Our categories:\n",
    "- ai: artificial intelligence\n",
    "- ml: machine learning\n",
    "- cv: computer vision\n",
    "- ds: data structures and algorithms\n",
    "\n",
    "I created a python file `collect_and_store_data.py` to fetch data from arXiv. Then I created PSQL database in my local. Send the data as `train` and `test`. Therefore, I won't have to deal with csv files and use my data from database. Also I created `read_data.py` to read data from PSQL database to my notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting data, we can work on it. When dealing with text data there is not much to look at to be honest. While the relationship between text and target may be clear to us here, we need to make sure the model understands it. So in `notebooks/data_analysis.ipynb` we take a look at some info about our data. Since I fetched the data I know that classes are balanced.\n",
    "\n",
    "You can see train and test set sizes:\n",
    "\n",
    "![](assets/traintestsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through API I got the categories ordered. First 1500 is ai, next 1500 is ml and so on. So I shuffled the data. While using ML algorithms it wouldn't be a problem but for deep learning it has to be done. \n",
    "\n",
    "There is a visualization of class distribution:\n",
    "\n",
    "![](assets/classdist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the `wordcloud` of every category to observe which words are used to most per category.\n",
    "\n",
    "![](assets/output.png)\n",
    "![](assets/output2.png)\n",
    "![](assets/output3.png)\n",
    "![](assets/output4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category contains at least two identical words.\n",
    "\n",
    "* Algorithm\n",
    "* Problem\n",
    "\n",
    "Considering the theme of the topics, it is normal for these words to appear in every paper abstract. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words in ML:\n",
    "- learning 2033\n",
    "- data 1498\n",
    "- algorithm 1218\n",
    "- problem 858\n",
    "- algorithms 837\n",
    "- model 763\n",
    "\n",
    "Most common words in AI:\n",
    "- paper 951\n",
    "- problem 772\n",
    "- algorithm 690\n",
    "- model 602\n",
    "- approach 595\n",
    "- problems 594\n",
    "\n",
    "Most common words in CV:\n",
    "- image 2039\n",
    "- images 1358\n",
    "- method 1155\n",
    "- paper 966\n",
    "- proposed 923\n",
    "- based 894\n",
    "\n",
    "Most common words in DS:\n",
    "- algorithm 2290\n",
    "- problem 2244\n",
    "- n 1879\n",
    "- time 1703\n",
    "- algorithms 1102\n",
    "- graph 948\n",
    "\n",
    "In CV you can see `image` and `images` in the first two. That's not ideal since they have the same meaning. Hence we should perform lemmetization or maybe stemming to the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can see word `algorithm` in each. Also `algorithm`. The rest are related words by each category. One thing is that in DS (Data Structures and Algorithms) you see k, n, 1. My guess is that DS topics usually contains this kinda reprentation letter for values. So seeing these letters is not that weird. 1 is also might be about dimensions. I bet there are 2 or 3 even. Let's see.\n",
    "\n",
    "`We consider the discrepancy problem of coloring $n$ intervals with $k$ colors\\nsuch that at each point on the line, the maximal difference between the number\\nof intervals of any two colors is minimal. Somewhat surprisingly, a coloring\\nwith maximal difference at most one always exists. Furthermore, we give an\\nalgorithm with running time $O(n \\\\log n + kn \\\\log k)$ for its construction.\\nThis is in particular interesting because many known results for discrepancy\\nproblems are non-constructive. This problem naturally models a load balancing\\nscenario, where $n$ tasks with given start- and endtimes have to be distributed\\namong $k$ servers. Our results imply that this can be done ideally balanced.\\n  When generalizing to $d$-dimensional boxes (instead of intervals), a solution\\nwith difference at most one is not always possible. We show that for any $d \\\\ge\\n2$ and any $k \\\\ge 2$ it is NP-complete to decide if such a solution exists,\\nwhich implies also NP-hardness of the respective minimization problem.\\n  In an online scenario, where intervals arrive over time and the color has to\\nbe decided upon arrival, the maximal difference in the size of color classes\\ncan become arbitrarily high for any online algorithm.`\n",
    "\n",
    "`\\n`'s are representing new line so ignore them but you can see `(n \\\\log n + kn \\\\log k)`. `n` are part of mathematical equations. So seeing n in top 20 is normal thinking the abstracts can likely have these kind of representations. So my idea is to keep those letters. It can help our model to find relations for specific category. There are digits too. Let's check what they represent. \n",
    "\n",
    "Another example:\n",
    "\n",
    "`Concept drift refers to a non stationary learning problem over time. The\\ntraining and the application data often mismatch in real life problems. In this\\nreport we present a context of concept drift problem 1. We focus on the issues\\nrelevant to adaptive training set formation. We present the framework and\\nterminology, and formulate a global picture of concept drift learners design.\\nWe start with formalizing the framework for the concept drifting data in\\nSection 1. In Section 2 we discuss the adaptivity mechanisms of the concept\\ndrift learners. In Section 3 we overview the principle mechanisms of concept\\ndrift learners. In this chapter we give a general picture of the available\\nalgorithms and categorize them based on their properties. Section 5 discusses\\nthe related research fields and Section 5 groups and presents major concept\\ndrift applications. This report is intended to give a bird's view of concept\\ndrift research field, provide a context of the research and position it within\\nbroad spectrum of research fields and applications.`\n",
    "\n",
    "In here `Section 2` it's been used to point out section. So we cannot say it's something that we might need but in the first example, it means something:\n",
    "\n",
    "`...factor less than 2...`\n",
    "\n",
    "But simply removing the numbers used for purposes such as specifying a section from the text means that we lose the numbers that indicate a topic related to the usage.\n",
    "\n",
    "In order not to lose such meanings, it would be much healthier to keep the numbers in the text. we should not lose the meanings that can lead us to the category. But we can convert them to their text representations. Like 2 -> two.\n",
    "\n",
    "Abstracts can be long texts. So let's look at the number of tokens contained in each abstract by category for train set:\n",
    "\n",
    "![](assets/output5.png)\n",
    "\n",
    "For test set:\n",
    "\n",
    "![](assets/output6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a distribution around 150. Our data is normally distributed. We can see abstracts containing less or more than 400 tokens in train and test. there is an estimated 500 samples in the test set. Our data doesn't have huge outliers, I'm relieved.\n",
    "\n",
    "While the token distribution in train is almost the same for each category, in the test set the AI category seems to be showing off in token maintenance.\n",
    "\n",
    "I thought abstracts wouldn't have website links in it. I decided to look to be safe than sorry. And I was right. There are not a lot but we should get rid of them. We'll do that in another notebook.\n",
    "\n",
    "`In recent years, quadratic weighted kappa has been growing in popularity in\\nthe machine learning community as an evaluation metric in domains where the\\ntarget labels to be predicted are drawn from integer ratings, usually obtained\\nfrom human experts. For example, it was the metric of choice in several recent,\\nhigh profile machine learning contests hosted on Kaggle :\\nhttps://www.kaggle.com/c/asap-aes , https://www.kaggle.com/c/asap-sas ,\\nhttps://www.kaggle.com/c/diabetic-retinopathy-detection . Yet, little is\\nunderstood about the nature of this metric, its underlying mathematical\\nproperties, where it fits among other common evaluation metrics such as mean\\nsquared error (MSE) and correlation, or if it can be optimized analytically,\\nand if so, how. Much of this is due to the cumbersome way that this metric is\\ncommonly defined. In this paper we first derive an equivalent but much simpler,\\nand more useful, definition for quadratic weighted kappa, and then employ this\\nalternate form to address the above issues.`\n",
    "\n",
    "Also abstracts are more likely to contain latin characters for some equations or any other representations. While creating a API, these are causing errors. In a way we should remove them from our texts. \n",
    "\n",
    "`In this paper, we study the two choice balls and bins process when balls are\\nnot allowed to choose any two random bins, but only bins that are connected by\\nan edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the\\ngraph is almost regular with degree $n^\\\\epsilon$, where $\\\\epsilon$ is not too\\nsmall, the previous bounds on the maximum load continue to hold. Precisely, the\\nmaximum load is $\\\\log \\\\log n + O(1/\\\\epsilon) + O(1)$. For general\\n$\\\\Delta$-regular graphs, we show that the maximum load is $\\\\log\\\\log n +\\nO(\\\\frac{\\\\log n}{\\\\log (\\\\Delta/\\\\log^4 n)}) + O(1)$ and also provide an almost\\nmatching lower bound of $\\\\log \\\\log n + \\\\frac{\\\\log n}{\\\\log (\\\\Delta \\\\log n)}$.\\n  V{\\\\\"o}cking [Voc99] showed that the maximum bin size with $d$ choice load\\nbalancing can be further improved to $O(\\\\log\\\\log n /d)$ by breaking ties to the\\nleft. This requires $d$ random bin choices. We show that such bounds can be\\nachieved by making only two random accesses and querying $d/2$ contiguous bins\\nin each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of\\n$d/2$ consecutive bins, if each ball chooses two groups at random and inserts\\nthe new ball into the least-loaded bin in the lesser loaded group, then the\\nmaximum load is $O(\\\\log\\\\log n/d)$ with high probability.`\n",
    "\n",
    "As for what we might need for our model, such representations are still on the math side, but they may not suggest as much logic in terms of operations as numbers or letters. Of course, we cannot say that they are absolutely useless, but we have to remove them for future problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's set the steps:\n",
    "- We will make the texts lower case.\n",
    "- We'll remove equations.\n",
    "- We'll remove website links.\n",
    "- we will convert digits to text\n",
    "- we will remove stopwords\n",
    "- we will remove latin characters\n",
    "- we will remove punctuations.\n",
    "- Stem and lemmatization.\n",
    "\n",
    "I am not sure yet about lemmatization. But since lemmatization doesn't cut the word abruptly like stemming, it cares about the meaning, I've written a function for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_equations(text):\n",
    "    # Remove digits and mathematical equations\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_accents(text):\n",
    "    # Remove accents from Latin letters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the processed words back into a sentence\n",
    "    processed_text = ' '.join(words)\n",
    "    return processed_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmatized words back into a sentence\n",
    "    processed_text = ' '.join(lemmatized_words)\n",
    "    return processed_text\n",
    "\n",
    "def number_to_text(text):\n",
    "    # Convert digits to text representation\n",
    "    p = inflect.engine()\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            words.append(p.number_to_words(word))\n",
    "        else:\n",
    "            words.append(word)\n",
    "    processed_text = ' '.join(words)\n",
    "    return processed_text\n",
    "\n",
    "def stem_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Initialize the Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Stem each word in the text\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the stemmed words back into a sentence\n",
    "    processed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def remove_website_links(text):\n",
    "    # Regular expression to match website links\n",
    "    processed_text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One data example:\n",
    "\n",
    "`We consider the classic budgeted maximum weight independent set (BMWIS) problem. The input is a graph G=(V,E), a weight function w:Vâ†’â„â‰¥0, a cost function c:Vâ†’â„â‰¥0, and a budget Bâˆˆâ„â‰¥0. The goal is to find an independent set SâŠ†V in G such that âˆ‘vâˆˆSc(v)â‰¤B, which maximizes the total weight âˆ‘vâˆˆSw(v). Since the problem on general graphs cannot be approximated within ratio |V|1âˆ’Îµ for any Îµ>0, BMWIS has attracted significant attention on graph families for which a maximum weight independent set can be computed in polynomial time. Two notable such graph families are bipartite and perfect graphs. BMWIS is known to be NP-hard on both of these graph families; however, the best possible approximation guarantees for these graphs are wide open.\n",
    "In this paper, we give a tight 2-approximation for BMWIS on perfect graphs and bipartite graphs. In particular, we give We a (2âˆ’Îµ) lower bound for BMWIS on bipartite graphs, already for the special case where the budget is replaced by a cardinality constraint, based on the Small Set Expansion Hypothesis (SSEH). For the upper bound, we design a 2-approximation for BMWIS on perfect graphs using a Lagrangian relaxation based technique. Finally, we obtain a tight lower bound for the capacitated maximum weight independent set (CMWIS) problem, the special case of BMWIS where w(v)=c(v) âˆ€vâˆˆV. We show that CMWIS on bipartite and perfect graphs is unlikely to admit an efficient polynomial-time approximation scheme (EPTAS). Thus, the existing PTAS for CMWIS is essentially the best we can expect.`\n",
    "\n",
    "You can see these figures âˆ‘,âˆˆ,Îµ,âˆ€,âŠ†. This figures are not unprocessible by FastAPI. We'll remove them before getting input. And such representations are still on the math side, but they may not suggest as much logic in terms of operations as numbers or letters. Of course, we cannot say that they are absolutely useless, but we have to remove them for future problems.\n",
    "\n",
    "After the preprocess steps:\n",
    "\n",
    "`consider classic budgeted maximum weight independent set bmwis problem input graph gve weight function wvR0 cost function cvR0 budget bR0 goal find independent set sv g vscvb maximizes total weight vswv since problem general graph approximated within ratio v1 zero bmwis attracted significant attention graph family maximum weight independent set computed polynomial time two notable graph family bipartite perfect graph bmwis known nphard graph family however best possible approximation guarantee graph wide open paper give tight 2approximation bmwis perfect graph bipartite graph particular give two lower bound bmwis bipartite graph already special case budget replaced cardinality constraint based small set expansion hypothesis sseh upper bound design 2approximation bmwis perfect graph using lagrangian relaxation based technique finally obtain tight lower bound capacitated maximum weight independent set cmwis problem special case bmwis wvcv vv show cmwis bipartite perfect graph unlikely admit efficient polynomialtime approximation scheme eptas thus existing ptas cmwis essentially best expect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used these algorithms for ML part:\n",
    "\n",
    "`LogisticRegression(),\n",
    "KNeighborsClassifier(),\n",
    "RandomForestClassifier(),\n",
    "XGBClassifier(),\n",
    "DecisionTreeClassifier(),\n",
    "SVC()`\n",
    "\n",
    "To decide I used `plotly` library to visualize `recall`, `precision`, `accuracy` and `f1 score`. And choose the first two outperformed model. \n",
    "\n",
    "I usually look at f1 score in ml algorithms since it harmonic mean of recall and precision. More accurate than accuracy.\n",
    "\n",
    "Machine learning algorithms can be more successful than neural networks because we have less data. It's a good idea to try it first before going to larger models. This was the most important part especially for this task. As I will point out shortly, NNs are very good at learning our train data, so I ran into an overfitting problem.\n",
    "\n",
    "These are the results that I get:\n",
    "\n",
    "![](assets/newplot1.png)\n",
    "![](assets/newplot2.png)\n",
    "![](assets/newplot3.png)\n",
    "![](assets/newplot4.png)\n",
    "![](assets/newplot5.png)\n",
    "![](assets/newplot6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I went for the best hyperparameters with grid search for Logistic and SVC to decide which one is the best.\n",
    "\n",
    "`SVC F1 Score -> 0.90889\n",
    "\n",
    "Logistic Regression -> 0.90828``\n",
    "\n",
    "SVC results with best hyperparameters:\n",
    "\n",
    "![](assets/newplot7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With grid search we find the best hyperparameters for our model and increased the score for %1.\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "![](assets/output7.png)\n",
    "\n",
    "Model has a weakness to call CV papers as ML papers followed by 36 with mistaken DS papers with AI papers. We can say the category with the lowest error rate is AI.\n",
    "\n",
    "Classification report:\n",
    "\n",
    "![](assets/output8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression results with best hyperparameters:\n",
    "\n",
    "![](assets/newplot8.png)\n",
    "\n",
    "Confussion matrix:\n",
    "\n",
    "![](assets/output9.png)\n",
    "\n",
    "Classification report:\n",
    "\n",
    "![](assets/output10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have almost the same scores. I looked at the darkest blues in confusion matrices. SVC's precision seems higher than logistic regression.\n",
    "\n",
    "Category-based errors in logistic regression are slightly higher. For example, the error rate in the `ds` category is higher in logistics, but in the results, we clearly see that `cv` papers are mixed with `ml` papers and `ds` papers are mixed with `ai` papers by the model. To be honest, it is necessary to look at very small things, but I need to choose the best one to compare with the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "We had good luck with the model on the ml side. they perform well enough on the last two models. As I mentioned at the beginning, we have little data and we are trying to make predictions about a wide-ranging subject. In such cases, neural networks should not be the first result to go. like I did.\n",
    "\n",
    "I applied the same preprocesses and tried various networks. I changed hyperparameters such as dimension, neuron, epoch vs. I encountered overfitting in every result. Finally, I tried to increase the train set by transferring a thousand data from the test meat to the train, but the result was the same.\n",
    "\n",
    "Let's break it down.\n",
    "\n",
    "I set vocab size and max seq length to pad our every input to same length.\n",
    "\n",
    "`MAX_NB_WORDS = 5000`\n",
    "\n",
    "`MAX_SEQUENCE_LENGTH = 400`\n",
    "\n",
    "Tokenize and pad the sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.stemmed_text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train.stemmed_text),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test.stemmed_text),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the targets:\n",
    "\n",
    "`y_train = tf.one_hot(train[\"target\"], depth=len(mapping))`\n",
    "\n",
    "`y_test = tf.one_hot(test[\"target\"], depth=len(mapping))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "layer = Embedding(vocab_size,50,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "layer = LSTM(64)(layer)\n",
    "layer = Dense(256,\"relu\",name='FC1')(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(4,\"softmax\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used f1 score and accuracy as metrics and Adam as optimizer.\n",
    "\n",
    "On the training set everything was great:\n",
    "\n",
    "![](assets/output11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks too good to be true till we see the evaluation on the test set:\n",
    "\n",
    "![](assets/output12.png)\n",
    "\n",
    "This model is not generaling on the unseen dataset. That's why the results are poorly on the test set.\n",
    "\n",
    "I increased the `embedding_dim` 50 to 200:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "layer = Embedding(vocab_size,200,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "layer = LSTM(64)(layer)\n",
    "layer = Dense(256,\"relu\",name='FC1')(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(4,\"softmax\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is no different with a huge overfitting:\n",
    "\n",
    "![](assets/output13.png)\n",
    "\n",
    "Evaluate:\n",
    "\n",
    "![](assets/output14.png)\n",
    "\n",
    "Even though I added `Dropout` layers to prevent overfitting but it didn't work.\n",
    "\n",
    "Then I thought this network might be a big for this kind of small dataset.\n",
    "\n",
    "Tried this network but had to train it on Kaggle since it did not work on my local:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_dim = 100 \n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "metric = tf.metrics.CategoricalAccuracy()\n",
    "opt = tf.keras.optimizers.legacy.Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy', metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/download.png)\n",
    "![](assets/download-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52 ml papers got mistaken as cv paper by the model. \n",
    "\n",
    "And as you can see that f1 score is %91 percent and the model's accuracy were about %98. f1 score and confusion matrix might look great but when you compare it with the train set score it is not that good. It's not something we would go for for best case. We can work on it to get better but still not what we need. \n",
    "\n",
    "Confusion matrix looks same with the SVC, it's because both perform almost same on test set.\n",
    "\n",
    "I also tried trained GloVE embeddings which has 400000 word vectors. By using Glove embeddings as features for our model, I aim to benefit from the rich information encoded in these embeddings and potentially improve the performance of model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMB = '/kaggle/input/glove-embeddings/glove.6B.300d.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(GLOVE_EMB)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "model = tf.keras.Model(sequence_input, outputs)\n",
    "\n",
    "metr = tf.metrics.CategoricalAccuracy()\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', metr])\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',    # Metric to monitor for early stopping (e.g., validation loss)\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model',   # Filepath to save the best model\n",
    "    monitor='val_loss',         # Metric to monitor for saving the best model\n",
    "    save_best_only=True,        # Save only the best model based on the monitored metric\n",
    "    save_weights_only=False,    # Save the entire model, including architecture and weights\n",
    "    verbose=1                   # Verbosity level: 0 (silent), 1 (progress bar), 2 (one line per epoch)\n",
    ")\n",
    "\n",
    "y_train = tf.one_hot(train[\"target\"], depth=len(mapping))\n",
    "y_test = tf.one_hot(test[\"target\"], depth=len(mapping))\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(x_test, y_test), callbacks=[early_stopping_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results:\n",
    "\n",
    "![](assets/glove_results4.png)\n",
    "\n",
    "While performing 0.9223 as accuracy preforms 0.8715 on test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](assets/glove_results.png)\n",
    "\n",
    "In confusion matrix cv paper's error rate is very high. 52 paper classified as ml paper. Also for AI, 51 paper classified as DS. Those error rates are a bit high.\n",
    "\n",
    "![](assets/glove_results2.png)\n",
    "\n",
    "![](assets/glove_results3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I increased `Dropout` to 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "layer = Embedding(vocab_size,50,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "layer = LSTM(64)(layer)\n",
    "layer = Dense(256,\"relu\",name='FC1')(layer)\n",
    "layer = Dropout(0.6)(layer)\n",
    "layer = Dense(128,\"relu\",name='FC2')(layer)\n",
    "layer = Dropout(0.6)(layer)\n",
    "layer = Dense(4,\"softmax\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set:\n",
    "\n",
    "> loss: 1.0026 - accuracy: 0.8155 - f1_score: 0.8162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True predictions are also decreased:\n",
    "\n",
    "![](assets/output15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I increased the train data by adding 1000 entry from test set. Tried the same networks. Not much changed. DS accurate estimates are noticeably less than the others, even with decreasing data. and ds error rate is also big compared to others. 50 samples estimated as AI.\n",
    "\n",
    "![](assets/increasedata.png)\n",
    "\n",
    "![](assets/increasedata2.png)\n",
    "\n",
    "Lastly I added `BatchNormalization` to normal data since nothing has changed. True predictions increased but error rate also.\n",
    "\n",
    "![](assets/increasedropout.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the left below corner. ML and DS categories are mistaken as AI paper and those values are not small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network for BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_dim = 100 \n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, \"relu\"))\n",
    "\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "metric = tf.metrics.CategoricalAccuracy()\n",
    "opt = tf.keras.optimizers.legacy.Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy', metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I tried quite a few things. I tried not to make the network too big. As it turned out, growing the model could have turned out much worse, as the model went from the task of understanding the data to memorizing.\n",
    "\n",
    "I added layers such as BatchNormalization, Dropout and played with hyperparameters. I did more of these but forgot to save the results from trying all the time ðŸ« \n",
    "\n",
    "Using more data, I think we could get a nice result from this problem with glove embeddings but for now glove embeddings also failed.\n",
    "\n",
    "Paper abstracts can sometimes contain many operations, and we have removed some components of these operations from texts. Such as punctuation, operation operators. Such removals may have damaged the semantic integrity of some texts. Increasing the number of data can also help the model to generalize in this sense.\n",
    "\n",
    "Finally, I decided to move forward with SVC by looking at the scores and the distribution of the predictions. Most of the papers I submitted on the site so far answered correctly, but of course I encountered a few mistakes. Let me talk a little bit about them.\n",
    "\n",
    "Example input is belong to DS category:\n",
    "\n",
    "`Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.`\n",
    "\n",
    "But model is saying it is `ML`. In my opinion, these errors occur in texts containing words such as `learning`, `linear`, `data` that are also included in the ML field a lot. Normally, DS contained unique words in our data. I have included the most frequently mentioned words above. In the abstract, seeing vocabs that are close to other categories other than their own vocab may lead to these inaccuracies.\n",
    "\n",
    "Another examples for the same topic:\n",
    "\n",
    "`The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the PÂ \n",
    "â‰  NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that justÂ \n",
    "adding a new state-action pair is considerably easier to implement.`\n",
    "\n",
    "`We study the problem of regret minimization for a single bidder in a sequence of first-price auctions where the bidder knows the item's value only if the auction is won. Our main contribution is a complete characterization, up to logarithmic factors, of the minimax regret in terms of the auction's transparency, which regulates the amount of information on competing bids disclosed by the auctioneer at the end of each auction. Our results hold under different assumptions (stochastic, adversarial, and their smoothed variants) on the environment generating the bidder's valuations and competing bids. These minimax rates reveal how the interplay between transparency and the nature of the environment affects how fast one can learn to bid optimally in first-price auctions.`\n",
    "\n",
    "According to what I have observed, the model usually puts ML labels on abstracts that do not contain formulas or numeric values and contain the words I mentioned above. But it gives the correct DS label to papers that contain words that are frequently encountered in our data, such as graph and time. This shows that most of the DS labeled abstracts in our train set contain such keywords a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not encounter many errors in other categories in my experiments. The DS was the most conspicuous. If I try harder, of course, I can find it, because the models can be improved as I mentioned.\n",
    "\n",
    "I hope the report was self explanatory ðŸ¤—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 64-bit ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de840a49bd71fe38c5570bae4d4048e5915ac8fdb56a470c0913411a02ea23b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
